{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stylegan_transfer_learning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-lkXLw5meMeLvAO_9lc3Vu5nytfGHrjH","authorship_tag":"ABX9TyNqWbN7u9wP9LNlFTBi3trz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KkPOADDu-ncr"},"source":["# StyleGAN2 전이학습을 이용한 학습방법과 이미지 생성 "]},{"cell_type":"markdown","metadata":{"id":"iVs7SiJ9-_k_"},"source":["**Content**\n","\n","*   데이터셋 모으기(image scraping)\n","*   얼굴 크롭&리사이즈\n","*   styleGAN 전이학습\n","*   이미지 생성\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c4RrJSsmACIE"},"source":["데이터는 구글, 네이버, 핀터레스트 등 이미지 스크레이핑함\n","\n","구글 비전 api로 얼굴인식 후 크롭\n","\n","스타일갠에서 학습할 수 있게 (256x256 or 512x512 or 1024x1024)로 리사이징까지 완료 \n","\n","이미지 파일 zip으로 묶어서 구글 드라이브에 준비"]},{"cell_type":"markdown","metadata":{"id":"sWZ86hQ2Ai6d"},"source":["# styleGAN 전이학습\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKC1MrHVMQLQ","executionInfo":{"status":"ok","timestamp":1636038557352,"user_tz":-540,"elapsed":570,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"0dc360c0-5ccc-42d2-a2a1-270b16ce816c"},"source":["# GPU환경에서만 작동, GPU사용 확인 \n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Nov  4 15:09:16 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWL_lgvYH84T","executionInfo":{"status":"ok","timestamp":1636038721202,"user_tz":-540,"elapsed":508,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"5167fb6e-1aba-4f34-9626-704c51005967"},"source":["# 텐서플로  1.x 에서만 동작하므로 버전 1로 연결 \n","%tensorflow_version 1.x"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"markdown","metadata":{"id":"kE3f8VbxBinl"},"source":["\n","**StyleGAN2-ADA repo 복사 및 학습할 이미지 불러오기 위해 구글드라이브 연결**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZdQtehvIAFZ","executionInfo":{"status":"ok","timestamp":1635911855191,"user_tz":-540,"elapsed":452,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"a590eb99-c1d3-41a5-dd56-34af1d1914df"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOMsOaKIMkI4","executionInfo":{"status":"ok","timestamp":1636038565932,"user_tz":-540,"elapsed":576,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"cb7a6510-d4bf-4afc-e935-70dbdb436cf2"},"source":["# StyleGAN2-ADA repo 있으면 불러오고 없으면 설치 \n","import os\n","if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada\"):\n","    %cd \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada\"\n","else:\n","    #install script\n","    %cd \"/content/drive/MyDrive/\"\n","    !mkdir colab-sg2-ada\n","    %cd colab-sg2-ada\n","    \n","    #fork of the official repo. \n","    !git clone https://github.com/dvschultz/stylegan2-ada\n","    %cd stylegan2-ada\n","    !mkdir downloads\n","    !mkdir datasets"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdBXYntDNB1F","executionInfo":{"status":"ok","timestamp":1635918753442,"user_tz":-540,"elapsed":4372,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"e31d4363-e34f-4092-dff4-2635493af820"},"source":["# 이미지 집파일 압축 풀기 \n","zip_path = \"/content/drive/MyDrive/gan/crop_resize.zip\"\n","!unzip {zip_path} -d /content/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/gan/crop_resize.zip\n","   creating: /content/crop_resize/\n","  inflating: /content/crop_resize/resized_crop_face36.jpg  \n","  inflating: /content/crop_resize/resized_crop_face22.jpg  \n","  inflating: /content/crop_resize/resized_crop_face134.jpg  \n","  inflating: /content/crop_resize/resized_crop_face120.jpg  \n","  inflating: /content/crop_resize/resized_crop_face108.jpg  \n","  inflating: /content/crop_resize/resized_crop_face109.jpg  \n","  inflating: /content/crop_resize/resized_crop_face121.jpg  \n","  inflating: /content/crop_resize/resized_crop_face135.jpg  \n","  inflating: /content/crop_resize/crop_face8_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face23.jpg  \n","  inflating: /content/crop_resize/resized_crop_face37.jpg  \n","  inflating: /content/crop_resize/resized_crop_face21.jpg  \n","  inflating: /content/crop_resize/resized_crop_face35.jpg  \n","  inflating: /content/crop_resize/resized_crop_face123.jpg  \n","  inflating: /content/crop_resize/resized_crop_face137.jpg  \n","  inflating: /content/crop_resize/crop_face13_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face136.jpg  \n","  inflating: /content/crop_resize/resized_crop_face122.jpg  \n","  inflating: /content/crop_resize/resized_crop_face20.jpg  \n","  inflating: /content/crop_resize/resized_crop_face24.jpg  \n","  inflating: /content/crop_resize/resized_crop_face30.jpg  \n","  inflating: /content/crop_resize/resized_crop_face18.jpg  \n","  inflating: /content/crop_resize/resized_crop_face126.jpg  \n","  inflating: /content/crop_resize/resized_crop_face132.jpg  \n","  inflating: /content/crop_resize/crop_face24_resized.jpg  \n","  inflating: /content/crop_resize/.DS_Store  \n","  inflating: /content/__MACOSX/crop_resize/._.DS_Store  \n","  inflating: /content/crop_resize/resized_crop_face133.jpg  \n","  inflating: /content/crop_resize/resized_crop_face127.jpg  \n","  inflating: /content/crop_resize/resized_crop_face19.jpg  \n","  inflating: /content/crop_resize/resized_crop_face31.jpg  \n","  inflating: /content/crop_resize/resized_crop_face25.jpg  \n","  inflating: /content/crop_resize/resized_crop_face33.jpg  \n","  inflating: /content/crop_resize/resized_crop_face27.jpg  \n","  inflating: /content/crop_resize/resized_crop_face119.jpg  \n","  inflating: /content/crop_resize/resized_crop_face131.jpg  \n","  inflating: /content/crop_resize/resized_crop_face125.jpg  \n","  inflating: /content/crop_resize/resized_crop_face124.jpg  \n","  inflating: /content/crop_resize/resized_crop_face130.jpg  \n","  inflating: /content/crop_resize/resized_crop_face118.jpg  \n","  inflating: /content/crop_resize/crop_face110_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face110_resized.jpg  \n","  inflating: /content/crop_resize/crop_face92_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face26.jpg  \n","  inflating: /content/crop_resize/resized_crop_face32.jpg  \n","  inflating: /content/crop_resize/resized_crop_face69.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_199.jpg  \n","  inflating: /content/crop_resize/resized_crop_face55.jpg  \n","  inflating: /content/crop_resize/resized_crop_face41.jpg  \n","  inflating: /content/crop_resize/resized_crop_face96.jpg  \n","  inflating: /content/crop_resize/resized_crop_face82.jpg  \n","  inflating: /content/crop_resize/crop_face113_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face113_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face157.jpg  \n","  inflating: /content/crop_resize/resized_crop_face143.jpg  \n","  inflating: /content/crop_resize/resized_crop_face180.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_210.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_204.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_205.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_211.jpg  \n","  inflating: /content/crop_resize/resized_crop_face181.jpg  \n","  inflating: /content/crop_resize/resized_crop_face142.jpg  \n","  inflating: /content/crop_resize/resized_crop_face156.jpg  \n","  inflating: /content/crop_resize/resized_crop_face83.jpg  \n","  inflating: /content/crop_resize/resized_crop_face97.jpg  \n","  inflating: /content/crop_resize/resized_crop_face40.jpg  \n","  inflating: /content/crop_resize/resized_crop_face54.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_198.jpg  \n","  inflating: /content/crop_resize/resized_crop_face68.jpg  \n","  inflating: /content/crop_resize/resized_crop_face42.jpg  \n","  inflating: /content/crop_resize/resized_crop_face56.jpg  \n","  inflating: /content/crop_resize/resized_crop_face81.jpg  \n","  inflating: /content/crop_resize/resized_crop_face95.jpg  \n","  inflating: /content/crop_resize/resized_crop_face140.jpg  \n","  inflating: /content/crop_resize/resized_crop_face154.jpg  \n","  inflating: /content/crop_resize/resized_crop_face168.jpg  \n","  inflating: /content/crop_resize/resized_crop_face183.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_207.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_213.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_212.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_206.jpg  \n","  inflating: /content/crop_resize/resized_crop_face182.jpg  \n","  inflating: /content/crop_resize/resized_crop_face169.jpg  \n","  inflating: /content/crop_resize/resized_crop_face155.jpg  \n","  inflating: /content/crop_resize/resized_crop_face141.jpg  \n","  inflating: /content/crop_resize/resized_crop_face94.jpg  \n","  inflating: /content/crop_resize/resized_crop_face80.jpg  \n","  inflating: /content/crop_resize/resized_crop_face57.jpg  \n","  inflating: /content/crop_resize/resized_crop_face43.jpg  \n","  inflating: /content/crop_resize/resized_crop_face47.jpg  \n","  inflating: /content/crop_resize/resized_crop_face53.jpg  \n","  inflating: /content/crop_resize/resized_crop_face84.jpg  \n","  inflating: /content/crop_resize/resized_crop_face90.jpg  \n","  inflating: /content/crop_resize/crop_face2_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face2_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face179.jpg  \n","  inflating: /content/crop_resize/resized_crop_face145.jpg  \n","  inflating: /content/crop_resize/resized_crop_face151.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._resized_crop_face151.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_202.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_216.jpg  \n","  inflating: /content/crop_resize/crop_face103_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face103_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face187.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_217.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_203.jpg  \n","  inflating: /content/crop_resize/resized_crop_face150.jpg  \n","  inflating: /content/crop_resize/resized_crop_face144.jpg  \n","  inflating: /content/crop_resize/resized_crop_face178.jpg  \n","  inflating: /content/crop_resize/resized_crop_face91.jpg  \n","  inflating: /content/crop_resize/resized_crop_face85.jpg  \n","  inflating: /content/crop_resize/resized_crop_face52.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._resized_crop_face52.jpg  \n","  inflating: /content/crop_resize/crop_face22_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face46.jpg  \n","  inflating: /content/crop_resize/resized_crop_face50.jpg  \n","  inflating: /content/crop_resize/resized_crop_face44.jpg  \n","  inflating: /content/crop_resize/resized_crop_face78.jpg  \n","  inflating: /content/crop_resize/resized_crop_face93.jpg  \n","  inflating: /content/crop_resize/resized_crop_face87.jpg  \n","  inflating: /content/crop_resize/resized_crop_face9.jpg  \n","  inflating: /content/crop_resize/resized_crop_face152.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._resized_crop_face152.jpg  \n","  inflating: /content/crop_resize/resized_crop_face146.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_215.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_201.jpg  \n","  inflating: /content/crop_resize/resized_crop_face184.jpg  \n","  inflating: /content/crop_resize/resized_crop_face190.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_200.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_214.jpg  \n","  inflating: /content/crop_resize/resized_crop_face147.jpg  \n","  inflating: /content/crop_resize/resized_crop_face153.jpg  \n","  inflating: /content/crop_resize/resized_crop_face8.jpg  \n","  inflating: /content/crop_resize/resized_crop_face86.jpg  \n","  inflating: /content/crop_resize/resized_crop_face92.jpg  \n","  inflating: /content/crop_resize/resized_crop_face79.jpg  \n","  inflating: /content/crop_resize/resized_crop_face45.jpg  \n","  inflating: /content/crop_resize/resized_crop_face51.jpg  \n","  inflating: /content/crop_resize/resized_crop_face48.jpg  \n","  inflating: /content/crop_resize/resized_crop_face74.jpg  \n","  inflating: /content/crop_resize/resized_crop_face60.jpg  \n","  inflating: /content/crop_resize/resized_crop_face5.jpg  \n","  inflating: /content/crop_resize/resized_crop_face176.jpg  \n","  inflating: /content/crop_resize/resized_crop_face162.jpg  \n","  inflating: /content/crop_resize/resized_crop_face189.jpg  \n","  inflating: /content/crop_resize/resized_crop_face188.jpg  \n","  inflating: /content/crop_resize/resized_crop_face163.jpg  \n","  inflating: /content/crop_resize/resized_crop_face177.jpg  \n","  inflating: /content/crop_resize/resized_crop_face4.jpg  \n","  inflating: /content/crop_resize/resized_crop_face61.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_191.jpg  \n","  inflating: /content/crop_resize/resized_crop_face75.jpg  \n","  inflating: /content/crop_resize/resized_crop_face49.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_193.jpg  \n","  inflating: /content/crop_resize/resized_crop_face63.jpg  \n","  inflating: /content/crop_resize/resized_crop_face77.jpg  \n","  inflating: /content/crop_resize/crop_face39_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face88.jpg  \n","  inflating: /content/crop_resize/resized_crop_face6.jpg  \n","  inflating: /content/crop_resize/resized_crop_face161.jpg  \n","  inflating: /content/crop_resize/resized_crop_face175.jpg  \n","  inflating: /content/crop_resize/crop_face17_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face149.jpg  \n","  inflating: /content/crop_resize/crop_face67_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face148.jpg  \n","  inflating: /content/crop_resize/resized_crop_face174.jpg  \n","  inflating: /content/crop_resize/resized_crop_face160.jpg  \n","  inflating: /content/crop_resize/resized_crop_face7.jpg  \n","  inflating: /content/crop_resize/resized_crop_face89.jpg  \n","  inflating: /content/crop_resize/resized_crop_face76.jpg  \n","  inflating: /content/crop_resize/resized_crop_face62.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_192.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_196.jpg  \n","  inflating: /content/crop_resize/resized_crop_face66.jpg  \n","  inflating: /content/crop_resize/resized_crop_face72.jpg  \n","  inflating: /content/crop_resize/resized_crop_face99.jpg  \n","  inflating: /content/crop_resize/resized_crop_face3.jpg  \n","  inflating: /content/crop_resize/resized_crop_face158.jpg  \n","  inflating: /content/crop_resize/resized_crop_face164.jpg  \n","  inflating: /content/crop_resize/resized_crop_face170.jpg  \n","  inflating: /content/crop_resize/crop_face20_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face171.jpg  \n","  inflating: /content/crop_resize/resized_crop_face165.jpg  \n","  inflating: /content/crop_resize/resized_crop_face159.jpg  \n","  inflating: /content/crop_resize/resized_crop_face2.jpg  \n","  inflating: /content/crop_resize/resized_crop_face98.jpg  \n","  inflating: /content/crop_resize/crop_face101_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face101_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face73.jpg  \n","  inflating: /content/crop_resize/resized_crop_face67.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_197.jpg  \n","  inflating: /content/crop_resize/resized_crop_face71.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_195.jpg  \n","  inflating: /content/crop_resize/resized_crop_face65.jpg  \n","  inflating: /content/crop_resize/resized_crop_face59.jpg  \n","  inflating: /content/crop_resize/resized_crop_face173.jpg  \n","  inflating: /content/crop_resize/resized_crop_face167.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_208.jpg  \n","  inflating: /content/crop_resize/crop_face45_resized.jpg  \n","  inflating: /content/crop_resize/crop_face29_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_209.jpg  \n","  inflating: /content/crop_resize/resized_crop_face166.jpg  \n","  inflating: /content/crop_resize/resized_crop_face1.jpg  \n","  inflating: /content/crop_resize/resized_crop_face58.jpg  \n","  inflating: /content/crop_resize/resized_crop_face64.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._resized_crop_face64.jpg  \n","  inflating: /content/crop_resize/resized_crop_face_194.jpg  \n","  inflating: /content/crop_resize/resized_crop_face70.jpg  \n","  inflating: /content/crop_resize/resized_crop_face17.jpg  \n","  inflating: /content/crop_resize/resized_crop_face115.jpg  \n","  inflating: /content/crop_resize/resized_crop_face101.jpg  \n","  inflating: /content/crop_resize/resized_crop_face129.jpg  \n","  inflating: /content/crop_resize/resized_crop_face128.jpg  \n","  inflating: /content/crop_resize/resized_crop_face100.jpg  \n","  inflating: /content/crop_resize/resized_crop_face114.jpg  \n","  inflating: /content/crop_resize/resized_crop_face16.jpg  \n","  inflating: /content/crop_resize/resized_crop_face28.jpg  \n","  inflating: /content/crop_resize/resized_crop_face14.jpg  \n","  inflating: /content/crop_resize/crop_face80_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face102.jpg  \n","  inflating: /content/crop_resize/resized_crop_face116.jpg  \n","  inflating: /content/crop_resize/resized_crop_face117.jpg  \n","  inflating: /content/crop_resize/resized_crop_face103.jpg  \n","  inflating: /content/crop_resize/resized_crop_face15.jpg  \n","  inflating: /content/crop_resize/resized_crop_face29.jpg  \n","  inflating: /content/crop_resize/resized_crop_face11.jpg  \n","  inflating: /content/crop_resize/resized_crop_face39.jpg  \n","  inflating: /content/crop_resize/resized_crop_face107.jpg  \n","  inflating: /content/crop_resize/resized_crop_face113.jpg  \n","  inflating: /content/crop_resize/crop_face85_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face85_resized.jpg  \n","  inflating: /content/crop_resize/crop_face14_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face112.jpg  \n","  inflating: /content/crop_resize/resized_crop_face106.jpg  \n","  inflating: /content/crop_resize/resized_crop_face38.jpg  \n","  inflating: /content/crop_resize/resized_crop_face10.jpg  \n","  inflating: /content/crop_resize/resized_crop_face12.jpg  \n","  inflating: /content/crop_resize/resized_crop_face138.jpg  \n","  inflating: /content/crop_resize/resized_crop_face110.jpg  \n","  inflating: /content/crop_resize/resized_crop_face104.jpg  \n","  inflating: /content/crop_resize/crop_face71_resized.jpg  \n","  inflating: /content/__MACOSX/crop_resize/._crop_face71_resized.jpg  \n","  inflating: /content/crop_resize/crop_face90_resized.jpg  \n","  inflating: /content/crop_resize/resized_crop_face105.jpg  \n","  inflating: /content/crop_resize/resized_crop_face111.jpg  \n","  inflating: /content/crop_resize/resized_crop_face139.jpg  \n","  inflating: /content/crop_resize/resized_crop_face13.jpg  \n"]}]},{"cell_type":"markdown","metadata":{"id":"DJ6ICMLfCZkN"},"source":["styleGAN에서 학습데이터는 tfrecord 포맷을 요구함 \n","\n","이미지 데이터를 .tfrecords 파일 포맷으로 변경\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zx_TIPDTNCXl","executionInfo":{"status":"ok","timestamp":1635918825210,"user_tz":-540,"elapsed":19728,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"f8a534e3-b961-4d22-e892-cfc8c28650cc"},"source":["# 변경할 데이터셋이 들어있는 경로 \n","dataset_path = \"/content/crop_resize\"\n","\n","# 변경 후 저장할 폴더이름 \n","dataset_name = \"tfrecord_img\"\n","\n","!python dataset_tool.py create_from_images ./datasets/{dataset_name} {dataset_path}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading images from \"/content/crop_resize\"\n","Creating dataset \"./datasets/tfrecord_img\"\n","0 / 234\rdataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n","  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n","Added 234 images.\n"]}]},{"cell_type":"code","metadata":{"id":"gb9e0uHEhcGC","executionInfo":{"status":"ok","timestamp":1636038986668,"user_tz":-540,"elapsed":527,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}}},"source":["# 변경 후 저장할 폴더이름 \n","dataset_name = \"tfrecord_img\""],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbE6yA1YDqCI"},"source":["**전이학습에 필요한 기본옵션 지정**"]},{"cell_type":"code","metadata":{"id":"ppKDFr4DCQKL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636038877010,"user_tz":-540,"elapsed":2906,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"8ba7a217-4047-40ac-a0ce-52ce57135f9d"},"source":["# train에 가능한 옵션들 볼수 있는 명령어 \n","!python train.py --help"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n","                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n","                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n","                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n","                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n","                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n","                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n","                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n","                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n","                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n","\n","Train a GAN using the techniques described in the paper\n","\"Training Generative Adversarial Networks with Limited Data\".\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","\n","general options:\n","  --outdir DIR          Where to save the results (required)\n","  --gpus INT            Number of GPUs to use (default: 1 gpu)\n","  --snap INT            Snapshot interval (default: 50 ticks)\n","  --seed INT            Random seed (default: 1000)\n","  -n, --dry-run         Print training options and exit\n","\n","training dataset:\n","  --data PATH           Training dataset path (required)\n","  --res INT             Dataset resolution (default: highest available)\n","  --mirror BOOL         Augment dataset with x-flips (default: false)\n","  --mirrory BOOL        Augment dataset with y-flips (default: false)\n","  --use-raw BOOL        Use raw image dataset, i.e. created from\n","                        create_from_images_raw (default: False)\n","\n","metrics:\n","  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n","  --metricdata PATH     Dataset to evaluate metrics against (optional)\n","\n","base config:\n","  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n","                        Base config (default: auto)\n","  --lrate FLOAT         Override learning rate\n","  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n","                        for discriminator) (default: false)\n","  --gamma FLOAT         Override R1 gamma\n","  --nkimg INT           Override starting count\n","  --kimg INT            Override training duration\n","  --topk FLOAT          utilize top-k training\n","\n","discriminator augmentation:\n","  --aug {noaug,ada,fixed,adarv}\n","                        Augmentation mode (default: ada)\n","  --p FLOAT             Specify augmentation probability for --aug=fixed\n","  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n","  --initstrength INITSTRENGTH\n","                        Override ADA strength at start\n","  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n","                        Augmentation pipeline (default: bgc)\n","\n","comparison methods:\n","  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n","                        Comparison method (default: nocmethod)\n","  --dcap FLOAT          Multiplier for discriminator capacity\n","\n","transfer learning:\n","  --resume RESUME       Resume from network pickle (default: noresume)\n","  --freezed INT         Freeze-D (default: 0 discriminator layers)\n","\n","examples:\n","\n","  # Train custom dataset using 1 GPU.\n","  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n","\n","  # Train class-conditional CIFAR-10 using 2 GPUs.\n","  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n","      --cfg=cifar\n","\n","  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n","  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n","      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n","\n","  # Reproduce original StyleGAN2 config F.\n","  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n","      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n","\n","available base configs (--cfg):\n","  auto           Automatically select reasonable defaults based on resolution\n","                 and GPU count. Good starting point for new datasets.\n","  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n","  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n","  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n","  paper1024      Reproduce results for MetFaces at 1024x1024.\n","  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n","  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n","\n","transfer learning source networks (--resume):\n","  ffhq256        FFHQ trained at 256x256 resolution.\n","  ffhq512        FFHQ trained at 512x512 resolution.\n","  ffhq1024       FFHQ trained at 1024x1024 resolution.\n","  celebahq256    CelebA-HQ trained at 256x256 resolution.\n","  lsundog256     LSUN Dog trained at 256x256 resolution.\n","  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n","  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n","  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n","  brecahad512    BreCaHAD trained at 512x512 resolution.\n","  cifar10        CIFAR10 trained at 32x32 resolution.\n","  metfaces512    MetFaces trained at 512x512 resolution.\n","  <path or URL>  Custom network pickle.\n"]}]},{"cell_type":"code","metadata":{"id":"65KdU_AmQcMT","executionInfo":{"status":"ok","timestamp":1636038834912,"user_tz":-540,"elapsed":540,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}}},"source":["# 모델이 샘플과 .pkl 을 생성하는 빈도 \n","snapshot_count = 4\n","\n","# x축 미러링\n","mirrored = True\n","\n","# y축 미러링\n","mirroredY = False\n","\n","# 코랩은 gpu사용한계가 있기 때문에 메트릭 없이 하는것을 추천\n","metric_list = None\n","\n","# augments\n","augs = \"bgc\"\n","\n","# 첫 전이학습은 \"ffhq512\" 로 지정 \n","# 전이학습후 저장된 마지막 피클파일 경로로 지정하면 이어서 학습 가능 \n","resume_from = \"ffhq512\"\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRTVMDEnTF6y","outputId":"487a8ede-fc4d-4922-f8b9-2c856c5f8836"},"source":["# 옵션지정 후 학습시작 명령어 \n","!python train.py --outdir ./results --snap={snapshot_count} --cfg=11gb-gpu --data=./datasets/{dataset_name} --augpipe={augs} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from} --augpipe=\"bg\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 4294967296 bytes == 0x558a73428000 @  0x7f6400f22001 0x7f63fe12554f 0x7f63fe175b58 0x7f63fe179b17 0x7f63fe218203 0x558a6cdbb544 0x558a6cdbb240 0x558a6ce2f627 0x558a6ce29ced 0x558a6cdbd48c 0x558a6cdfe159 0x558a6cdfb0a4 0x558a6cdbbd49 0x558a6ce2f94f 0x558a6ce299ee 0x558a6ccfbe2b 0x558a6ce2bfe4 0x558a6ce299ee 0x558a6ccfbe2b 0x558a6ce2bfe4 0x558a6ce29ced 0x558a6ccfbe2b 0x558a6ce2bfe4 0x558a6cdbcafa 0x558a6ce2a915 0x558a6ce299ee 0x558a6ce296f3 0x558a6cef34c2 0x558a6cef383d 0x558a6cef36e6 0x558a6cecb163\n","tcmalloc: large alloc 4294967296 bytes == 0x558b73428000 @  0x7f6400f201e7 0x7f63fe12546e 0x7f63fe175c7b 0x7f63fe17635f 0x7f63fe218103 0x558a6cdbb544 0x558a6cdbb240 0x558a6ce2f627 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6cdbcafa 0x558a6ce2a915 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2ed00 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6ce29ced 0x558a6cdbd48c 0x558a6cdfe159 0x558a6cdfb0a4 0x558a6cdbbd49 0x558a6ce2f94f\n","tcmalloc: large alloc 4294967296 bytes == 0x558c74d2c000 @  0x7f6400f201e7 0x7f63fe12546e 0x7f63fe175c7b 0x7f63fe17635f 0x7f63bab27235 0x7f63ba4aa792 0x7f63ba4aad42 0x7f63ba463aee 0x558a6cdbb437 0x558a6cdbb240 0x558a6ce2f0f3 0x558a6cdbcafa 0x558a6ce2ac0d 0x558a6ce29ced 0x558a6ccfbeb0 0x558a6ce2bfe4 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2ac0d 0x558a6ce29ced 0x558a6cdbcbda 0x558a6ce2ac0d 0x558a6cdbcafa 0x558a6ce2ac0d 0x558a6ce299ee 0x558a6cdbd271 0x558a6cdbd698 0x558a6ce2bfe4 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2a915\n","\n","Training options:\n","{\n","  \"G_args\": {\n","    \"func_name\": \"training.networks.G_main\",\n","    \"fmap_base\": 16384,\n","    \"fmap_max\": 512,\n","    \"mapping_layers\": 8,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"D_args\": {\n","    \"func_name\": \"training.networks.D_main\",\n","    \"mbstd_group_size\": 4,\n","    \"fmap_base\": 16384,\n","    \"fmap_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_args\": {\n","    \"beta1\": 0.0,\n","    \"beta2\": 0.99,\n","    \"learning_rate\": 0.002\n","  },\n","  \"D_opt_args\": {\n","    \"beta1\": 0.0,\n","    \"beta2\": 0.99,\n","    \"learning_rate\": 0.002\n","  },\n","  \"loss_args\": {\n","    \"func_name\": \"training.loss.stylegan2\",\n","    \"r1_gamma\": 10\n","  },\n","  \"augment_args\": {\n","    \"class_name\": \"training.augment.AdaptiveAugment\",\n","    \"tune_heuristic\": \"rt\",\n","    \"tune_target\": 0.6,\n","    \"apply_func\": \"training.augment.augment_pipeline\",\n","    \"apply_args\": {\n","      \"xflip\": 1,\n","      \"rotate90\": 1,\n","      \"xint\": 1,\n","      \"scale\": 1,\n","      \"rotate\": 1,\n","      \"aniso\": 1,\n","      \"xfrac\": 1\n","    },\n","    \"tune_kimg\": 100\n","  },\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 4,\n","  \"network_snapshot_ticks\": 4,\n","  \"train_dataset_args\": {\n","    \"path\": \"./datasets/tfrecord_img\",\n","    \"max_label_size\": 0,\n","    \"use_raw\": false,\n","    \"resolution\": 512,\n","    \"mirror_augment\": true,\n","    \"mirror_augment_v\": false\n","  },\n","  \"metric_arg_list\": [],\n","  \"metric_dataset_args\": {\n","    \"path\": \"./datasets/tfrecord_img\",\n","    \"max_label_size\": 0,\n","    \"use_raw\": false,\n","    \"resolution\": 512,\n","    \"mirror_augment\": true,\n","    \"mirror_augment_v\": false\n","  },\n","  \"total_kimg\": 25000,\n","  \"minibatch_size\": 4,\n","  \"minibatch_gpu\": 4,\n","  \"G_smoothing_kimg\": 10,\n","  \"G_smoothing_rampup\": null,\n","  \"resume_pkl\": \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00001-tfrecord_img-mirror-11gb-gpu-bg-resumeffhq512/network-snapshot-000080.pkl\",\n","  \"run_dir\": \"./results/00002-tfrecord_img-mirror-11gb-gpu-bg-resumecustom\"\n","}\n","\n","Output directory:  ./results/00002-tfrecord_img-mirror-11gb-gpu-bg-resumecustom\n","Training data:     ./datasets/tfrecord_img\n","Training length:   25000 kimg\n","Resolution:        512\n","Number of GPUs:    1\n","\n","Creating output directory...\n","Loading training set...\n","tcmalloc: large alloc 4294967296 bytes == 0x558a73366000 @  0x7f6400f22001 0x7f63fe12554f 0x7f63fe175b58 0x7f63fe179b17 0x7f63fe218203 0x558a6cdbb544 0x558a6cdbb240 0x558a6ce2f627 0x558a6ce29ced 0x558a6cdbd48c 0x558a6cdfe159 0x558a6cdfb0a4 0x558a6cdbbd49 0x558a6ce2f94f 0x558a6ce299ee 0x558a6ccfbe2b 0x558a6ce2bfe4 0x558a6ce299ee 0x558a6ccfbe2b 0x558a6ce2bfe4 0x558a6ce29ced 0x558a6ccfbe2b 0x558a6ce2bfe4 0x558a6cdbcafa 0x558a6ce2a915 0x558a6ce299ee 0x558a6ce296f3 0x558a6cef34c2 0x558a6cef383d 0x558a6cef36e6 0x558a6cecb163\n","tcmalloc: large alloc 4294967296 bytes == 0x558d74d2c000 @  0x7f6400f201e7 0x7f63fe12546e 0x7f63fe175c7b 0x7f63fe17635f 0x7f63fe218103 0x558a6cdbb544 0x558a6cdbb240 0x558a6ce2f627 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6cdbcafa 0x558a6ce2a915 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2ed00 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2b737 0x558a6ce29ced 0x558a6cdbd48c 0x558a6cdfe159 0x558a6cdfb0a4 0x558a6cdbbd49 0x558a6ce2f94f\n","tcmalloc: large alloc 4294967296 bytes == 0x558d74d2c000 @  0x7f6400f201e7 0x7f63fe12546e 0x7f63fe175c7b 0x7f63fe17635f 0x7f63bab27235 0x7f63ba4aa792 0x7f63ba4aad42 0x7f63ba463aee 0x558a6cdbb437 0x558a6cdbb240 0x558a6ce2f0f3 0x558a6cdbcafa 0x558a6ce2ac0d 0x558a6ce29ced 0x558a6ccfbeb0 0x558a6ce2bfe4 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2ac0d 0x558a6ce29ced 0x558a6cdbcbda 0x558a6ce2ac0d 0x558a6cdbcafa 0x558a6ce2ac0d 0x558a6ce299ee 0x558a6cdbd271 0x558a6cdbd698 0x558a6ce2bfe4 0x558a6ce299ee 0x558a6cdbcbda 0x558a6ce2a915\n","Image shape: [3, 512, 512]\n","Label shape: [0]\n","\n","Constructing networks...\n","Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Loading... Done.\n","Setting up TensorFlow plugin \"upfirdn_2d.cu\": Compiling... Loading... Done.\n","Resuming from \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00001-tfrecord_img-mirror-11gb-gpu-bg-resumeffhq512/network-snapshot-000080.pkl\"\n","\n","G                             Params    OutputShape         WeightShape     \n","---                           ---       ---                 ---             \n","latents_in                    -         (?, 512)            -               \n","labels_in                     -         (?, 0)              -               \n","epochs                        1         ()                  ()              \n","epochs_1                      1         ()                  ()              \n","G_mapping/Normalize           -         (?, 512)            -               \n","G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense2              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense3              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense4              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense5              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense6              262656    (?, 512)            (512, 512)      \n","G_mapping/Dense7              262656    (?, 512)            (512, 512)      \n","G_mapping/Broadcast           -         (?, 16, 512)        -               \n","dlatent_avg                   -         (512,)              -               \n","Truncation/Lerp               -         (?, 16, 512)        -               \n","G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n","G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n","G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n","G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n","G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n","G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n","G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n","G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n","G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n","G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n","G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n","G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n","G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n","G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n","G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n","G_synthesis/64x64/Conv0_up    2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n","G_synthesis/64x64/Conv1       2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n","G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n","G_synthesis/64x64/ToRGB       264195    (?, 3, 64, 64)      (1, 1, 512, 3)  \n","G_synthesis/128x128/Conv0_up  1442561   (?, 256, 128, 128)  (3, 3, 512, 256)\n","G_synthesis/128x128/Conv1     721409    (?, 256, 128, 128)  (3, 3, 256, 256)\n","G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n","G_synthesis/128x128/ToRGB     132099    (?, 3, 128, 128)    (1, 1, 256, 3)  \n","G_synthesis/256x256/Conv0_up  426369    (?, 128, 256, 256)  (3, 3, 256, 128)\n","G_synthesis/256x256/Conv1     213249    (?, 128, 256, 256)  (3, 3, 128, 128)\n","G_synthesis/256x256/Upsample  -         (?, 3, 256, 256)    -               \n","G_synthesis/256x256/ToRGB     66051     (?, 3, 256, 256)    (1, 1, 128, 3)  \n","G_synthesis/512x512/Conv0_up  139457    (?, 64, 512, 512)   (3, 3, 128, 64) \n","G_synthesis/512x512/Conv1     69761     (?, 64, 512, 512)   (3, 3, 64, 64)  \n","G_synthesis/512x512/Upsample  -         (?, 3, 512, 512)    -               \n","G_synthesis/512x512/ToRGB     33027     (?, 3, 512, 512)    (1, 1, 64, 3)   \n","---                           ---       ---                 ---             \n","Total                         30276585                                      \n","\n","\n","D                    Params    OutputShape         WeightShape     \n","---                  ---       ---                 ---             \n","images_in            -         (?, 3, 512, 512)    -               \n","labels_in            -         (?, 0)              -               \n","512x512/FromRGB      256       (?, 64, 512, 512)   (1, 1, 3, 64)   \n","512x512/Conv0        36928     (?, 64, 512, 512)   (3, 3, 64, 64)  \n","512x512/Conv1_down   73856     (?, 128, 256, 256)  (3, 3, 64, 128) \n","512x512/Skip         8192      (?, 128, 256, 256)  (1, 1, 64, 128) \n","256x256/Conv0        147584    (?, 128, 256, 256)  (3, 3, 128, 128)\n","256x256/Conv1_down   295168    (?, 256, 128, 128)  (3, 3, 128, 256)\n","256x256/Skip         32768     (?, 256, 128, 128)  (1, 1, 128, 256)\n","128x128/Conv0        590080    (?, 256, 128, 128)  (3, 3, 256, 256)\n","128x128/Conv1_down   1180160   (?, 512, 64, 64)    (3, 3, 256, 512)\n","128x128/Skip         131072    (?, 512, 64, 64)    (1, 1, 256, 512)\n","64x64/Conv0          2359808   (?, 512, 64, 64)    (3, 3, 512, 512)\n","64x64/Conv1_down     2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n","64x64/Skip           262144    (?, 512, 32, 32)    (1, 1, 512, 512)\n","32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n","32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n","32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n","16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n","16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n","16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n","8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n","8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n","8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n","4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n","4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n","4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n","Output               513       (?, 1)              (512, 1)        \n","---                  ---       ---                 ---             \n","Total                28982849                                      \n","\n","Exporting sample images...\n","Replicating networks across 1 GPUs...\n","Initializing augmentations...\n","Setting up optimizers...\n","Constructing training graph...\n","Finalizing training ops...\n","Initializing metrics...\n","Training for 25000 kimg...\n","\n","tick 0     kimg 0.0      time 2m 02s       sec/tick 20.7    sec/kimg 1291.40 maintenance 101.1  gpumem 7.1   augment 0.000\n","tick 1     kimg 4.0      time 21m 12s      sec/tick 1130.9  sec/kimg 282.71  maintenance 19.2   gpumem 7.1   augment 0.037\n","tick 2     kimg 8.0      time 40m 04s      sec/tick 1132.3  sec/kimg 283.06  maintenance 0.0    gpumem 7.1   augment 0.074\n","tick 3     kimg 12.0     time 58m 58s      sec/tick 1133.5  sec/kimg 283.37  maintenance 0.0    gpumem 7.1   augment 0.109\n","tick 4     kimg 16.0     time 1h 17m 54s   sec/tick 1136.6  sec/kimg 284.14  maintenance 0.0    gpumem 7.1   augment 0.147\n","tick 5     kimg 20.0     time 1h 37m 00s   sec/tick 1138.1  sec/kimg 284.52  maintenance 7.9    gpumem 7.1   augment 0.182\n","tick 6     kimg 24.0     time 1h 56m 01s   sec/tick 1140.7  sec/kimg 285.18  maintenance 0.0    gpumem 7.1   augment 0.214\n","tick 7     kimg 28.0     time 2h 15m 02s   sec/tick 1141.2  sec/kimg 285.30  maintenance 0.0    gpumem 7.1   augment 0.243\n","tick 8     kimg 32.0     time 2h 34m 04s   sec/tick 1142.1  sec/kimg 285.53  maintenance 0.0    gpumem 7.1   augment 0.273\n","tick 9     kimg 36.0     time 2h 53m 16s   sec/tick 1144.3  sec/kimg 286.08  maintenance 7.2    gpumem 7.1   augment 0.296\n","tick 10    kimg 40.0     time 3h 12m 21s   sec/tick 1145.5  sec/kimg 286.38  maintenance 0.0    gpumem 7.1   augment 0.316\n","tick 11    kimg 44.0     time 3h 31m 26s   sec/tick 1144.6  sec/kimg 286.14  maintenance 0.0    gpumem 7.1   augment 0.338\n","tick 12    kimg 48.0     time 3h 50m 32s   sec/tick 1146.4  sec/kimg 286.60  maintenance 0.0    gpumem 7.1   augment 0.358\n","tick 13    kimg 52.0     time 4h 09m 46s   sec/tick 1146.2  sec/kimg 286.55  maintenance 7.0    gpumem 7.1   augment 0.378\n","tick 14    kimg 56.0     time 4h 28m 53s   sec/tick 1147.2  sec/kimg 286.80  maintenance 0.0    gpumem 7.1   augment 0.393\n","tick 15    kimg 60.0     time 4h 48m 01s   sec/tick 1148.6  sec/kimg 287.14  maintenance 0.0    gpumem 7.1   augment 0.407\n","tick 16    kimg 64.0     time 5h 07m 09s   sec/tick 1147.4  sec/kimg 286.86  maintenance 0.0    gpumem 7.1   augment 0.420\n","tick 17    kimg 68.0     time 5h 26m 24s   sec/tick 1148.2  sec/kimg 287.05  maintenance 7.2    gpumem 7.1   augment 0.435\n","tick 18    kimg 72.0     time 5h 45m 34s   sec/tick 1149.4  sec/kimg 287.34  maintenance 0.0    gpumem 7.1   augment 0.448\n","tick 19    kimg 76.0     time 6h 04m 43s   sec/tick 1149.4  sec/kimg 287.34  maintenance 0.0    gpumem 7.1   augment 0.462\n","tick 20    kimg 80.0     time 6h 23m 54s   sec/tick 1150.7  sec/kimg 287.68  maintenance 0.0    gpumem 7.1   augment 0.473\n","tick 21    kimg 84.0     time 6h 43m 11s   sec/tick 1150.4  sec/kimg 287.60  maintenance 7.3    gpumem 7.1   augment 0.484\n","tick 22    kimg 88.0     time 7h 02m 23s   sec/tick 1151.2  sec/kimg 287.80  maintenance 0.0    gpumem 7.1   augment 0.498\n","tick 23    kimg 92.0     time 7h 21m 34s   sec/tick 1151.3  sec/kimg 287.83  maintenance 0.0    gpumem 7.1   augment 0.510\n","tick 24    kimg 96.0     time 7h 40m 46s   sec/tick 1152.1  sec/kimg 288.02  maintenance 0.0    gpumem 7.1   augment 0.520\n","tick 25    kimg 100.0    time 8h 00m 06s   sec/tick 1152.1  sec/kimg 288.02  maintenance 7.4    gpumem 7.1   augment 0.531\n","tick 26    kimg 104.0    time 8h 19m 18s   sec/tick 1152.8  sec/kimg 288.20  maintenance 0.0    gpumem 7.1   augment 0.540\n"]}]},{"cell_type":"markdown","metadata":{"id":"Qp9ARKQaEbzH"},"source":["모델이 학습하면서 체크포인트마다 .pkl 파일과 .jpg 파일 출력\n","\n","jpg파일은 지금까지 학습한 모델을 사용해 변환시킨 이미지를 보여줌 "]},{"cell_type":"markdown","metadata":{"id":"PoNd0dhl9jw4"},"source":["# 이미지 생성 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BqE8IWJ8s5p","executionInfo":{"status":"ok","timestamp":1635911918368,"user_tz":-540,"elapsed":3640,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"77437435-a250-480f-88a3-162797765cdf"},"source":["# 이미지 생성에 필요한 라이브러리 설치 \n","!pip install opensimplex"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opensimplex\n","  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n","Installing collected packages: opensimplex\n","Successfully installed opensimplex-0.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"FcY9H1DdFsGf"},"source":["\n","**generate.py** : 시드별 이미지 생성\n","\n","seeds : 저장된 시드별 오리지널이미지에 학습한 모델의 스타일을 입힐수 있음 \n","\n","\n","(10, 20 단일지정 or 0-30 처럼 범위지정)\n","\n","network : 학습한 모델(.pkl file)경로\n","\n","시드와 모델 지정후 이미지 generate > out 폴더에 저장 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cUH8talb8tdJ","executionInfo":{"status":"ok","timestamp":1635912075122,"user_tz":-540,"elapsed":53393,"user":{"displayName":"Hajin Jo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjn9lShorMHWWQ3ToBUNoxPb7xspgl8Sm7mjfHM9w=s64","userId":"14849470763389388295"}},"outputId":"86ad6507-a945-46cf-e995-7c4bf56157fe"},"source":["!python generate.py generate-images --trunc=1 --seeds=85,265,297,849 --network=/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00000-tf_img-mirror-11gb-gpu-bg-resumeffhq512/network-snapshot-000080.pkl"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n","Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b360448/45929032 bytes (0.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2932736/45929032 bytes (6.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6987776/45929032 bytes (15.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11182080/45929032 bytes (24.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15335424/45929032 bytes (33.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19415040/45929032 bytes (42.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23470080/45929032 bytes (51.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27451392/45929032 bytes (59.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31399936/45929032 bytes (68.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35266560/45929032 bytes (76.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39321600/45929032 bytes (85.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43376640/45929032 bytes (94.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n","Loading networks from \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00000-tf_img-mirror-11gb-gpu-bg-resumeffhq512/network-snapshot-000044.pkl\"...\n","Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Loading... Done.\n","Setting up TensorFlow plugin \"upfirdn_2d.cu\": Compiling... Loading... Done.\n","Generating image for seed 85 (0/4) ...\n","Generating image for seed 265 (1/4) ...\n","Generating image for seed 297 (2/4) ...\n","Generating image for seed 849 (3/4) ...\n"]}]},{"cell_type":"markdown","metadata":{"id":"QOtTHpA2zbAj"},"source":["**projector.py** : 내가 원하는 이미지에 styleGAN입힌 이미지 생성 \n","\n","outdir : 이미지 저장될 위치 \n","\n","target : 변경할 이미지 경로\n","\n","save_video : 변환과정 비디오 저장 여부 \n","\n","\n","실행하면 변환된 이미지와 dlatents.npz 파일 저장 \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"95Sb-ZE_zZP3"},"source":["!python projector.py --outdir=out --target=/content/drive/MyDrive/test_img/resized_crop_face9.jpg \\\n","    --save-video=False --network=/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00001-tfrecord_img-mirror-11gb-gpu-bg-resumeffhq512/network-snapshot-000080.pkl\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNm5p_pI1Wku"},"source":["dlatents.npz파일을 사용해 generate.py 에서 dlatents 경로지정하면 같은 이미지 재생성 가능 "]},{"cell_type":"code","metadata":{"id":"TgaCXLNC1R1u"},"source":["!python generate.py --outdir=out --dlatents=out/dlatents.npz \\ # dlatents 지정\n","    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5fNJmstC1jpl"},"source":["**style_mixing.py** : 좀 더 세부적인 스타일 변경\n","\n","rows, cols seed를 지정해서 스타일 믹싱 "]},{"cell_type":"code","metadata":{"id":"pk6ffi5a1irH"},"source":["!python style_mixing.py --outdir=out --rows=1 --cols=2 \\\n","--network=/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00001-tfrecord_img-mirror-11gb-gpu-bg-resumeffhq512/network-snapshot-000080.pkl"],"execution_count":null,"outputs":[]}]}